---
title: "有序logistic回归分析"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ordinal_logistic}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

这里我们学习一下，如何使用`R`中的`MASS::poly`来进行有序logistic回归分析。有序logistic回归分析面向的数据：因变量$Y$是有序分类的，且类别数量超过2个。这里，我们称类别为等级。进行有序logistic回归分析的模型有很多，其中最常使用的是比例优势模型（Proportional Odds Model，POM），我们介绍如下。

# 比例优势模型

这里假设$Y$的等级数量为$J$，其取值为$\{1,\cdots,J\}$，则我们可以定义累计概率$P(Y\le j)$，其中$P(Y\le J)=1$。进一步，我们可以定义小于等于**某一个等级的odds**为：
$$\frac{P(Y\le j)}{P(Y\gt j)},\quad j=1,\cdots,J-1$$
因为$P(Y\gt J)=0$，所以我们无法定义$J$的odds。

> 如果我们将$P(Y\gt j)$写成$1-P(Y\le j)$，则这里的odds的定义和binary logistic regression中的定义是一致的。

进一步，我们定义**logit**，也就是**log odds**为：
$$logit(P(Y\le j))=\log {\frac{P(Y\le j)}{P(Y\gt j)}}\quad j=1,\cdots,J-1$$
以上的公式我们可以称之为是**分点$j$的logit**。由此logit，广义的ordinal logistic regression可以定义为：
$$logit(P(Y\le j))=\beta_{j0}+\beta_{j1}x_1+\cdots+\beta_{jp}x_p$$

假设
$$\beta_{1k}=\beta_{2k}=\cdots=\beta_{(J-1)k}$$
则我们可以使用相同的$\beta$来描述每一个分点的logit，模型的参数数量大大减小（$(J-1)\times(p+1)\to(J+p-1)$，这就是比例优势模型POM。而这个假设称为**比例优势假设（Parallel lines assumption、Parallel slope assumption或Proportional Odds assumption，PLA）**。POM模型可以描述如下：
$$logit(P(Y\le j))=\beta_{j0}+\beta_{1}x_1+\cdots+\beta_{p}x_p$$
大多数时候，上述模型被等价表示为：
$$logit(P(Y\le j))=\beta_{j0}-\eta_{1}x_1+\cdots-\eta_{p}x_p$$
其中$\eta_i=-\beta_i$。

## 模型参数解释

若只有一个自变量$x$，则：

$$
\begin{align}
\eta=-\beta&=-[logit(P(Y\le j|x+1))-logit(P(Y\le j|x))] \\
&=-log[\frac{P(Y\le j|x+1)}{P(Y\gt j|x+1)}/\frac{P(Y\le j|x)}{P(Y\gt j|x)}] \\
&=-log[\frac{P(Y\le j|x+1)}{P(Y\le j|x)}/\frac{P(Y\gt j|x+1)}{P(Y\gt j|x)}] \\
&=log[\frac{P(Y\gt j|x+1)}{P(Y\le j|x+1)}/\frac{P(Y\gt j|x)}{P(Y\le j|x)}]
\end{align}
$$
我们可以看到，通过变换，这里的解释有两种角度：分别从不同因变量水平和不同自变量水平来看。

从不同因变量水平来看，$exp(\eta)$值表示的是高水平因变量在不同自变量水平下的odds（$\frac{P(Y\gt j|x+1)}{P(Y\gt j|x)}$）相比于低水平因变量在不同自变量水平下odds（$\frac{P(Y\le j|x+1)}{P(Y\le j|x)}$）的倍数。

从不同自变量水平来看，$exp(\eta)$值表示的每提高一个单位的自变量$x$，所导致的分点的odds的升高的倍数。

> 从不同自变量水平来解释更加自然一些，因为这种叙述方式体现了一种自变量变化导致因变量变化的因果关系。

## R实例分析

这里我们使用`MASS`中的`poly`函数来进行POM的建模。

```{r setup}
library(misc)
library(MASS)
library(foreign)
```

这里使用一个模拟数据集，其有4个变量：

* apply，因变量，3-levels的有序变量，levels是“unlikely”、“somewhat likely”和“very likely”，表示学生是否希望申请攻读研究生。
* pared，自变量，2-levels，表示父母中的一人是否拥有有研究生学位。
* public，自变量，2-levels，表示本科院校是否是公立的。
* gpa，自变量，连续变量，学生的平均成绩。

这些数据展示如下：

```{r}
ologit <- read.dta("https://stats.idre.ucla.edu/stat/data/ologit.dta")
# data(ologit)
# ologit <- load(file = "../data/ologit.rda")
head(ologit)
```

```{r}
# 分类变量的边际分布情况：
lapply(ologit[, c("apply", "pared", "public")], table)
```

```{r}
# 分类变量的联合分布情况
ftable(xtabs(~ public + apply + pared, data = ologit))
```

```{r}
# 分类变量gpa的联合分布情况
summary(ologit$gpa)
```

```{r}
# 分类变量gpa的标准差
sd(ologit$gpa)
```

拟合POM模型：

```{r}
fit <- polr(apply ~ ., data = ologit, Hess = TRUE)  # Hess=TRUE表示返回信息矩阵
summary(fit)
```

上述结果解释如下：

* Coefficients和Intercepts是参数估计的结果，这里默认没有提供p值。Coefficients提供的是$\eta_i,i\ge1$，Intercepts提供的是$\beta_{0j},j=1\cdots,J-1$。
* 最后有residual deviance（就是$-2\log(likelihood)$）和AIC（$2k-2\log(likelihood)$，$k$表示要拟合的参数数量）来对模型的拟合优度进行评价。

根据上面的结果，我们知道，拟合得到的模型是下面这样的：
$$logit(P(Y\le 1))=2.20 - 1.05 * pared + 0.06 * public - 0.616 * gpa$$
$$logit(P(Y\le 2))=4.30 - 1.05 * pared + 0.06 * public - 0.616 * gpa$$
**p 值**

上面的结果没有p值，一个得到p值的方法是使用上面的`t values`去计算p值。当样本量较大（自由度较大）时，`t values`近似服从标准正态分布，所以可以进行如下计算：

```{r}
# coef(fit)仅仅只能得到coefficient，而coef(summary(fit))除了得到coefficient外，还有它的标准误和t values
# coef(fit)
ctable <- coef(summary(fit))
ctable
```

```{r}
# 计算p值
p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
ctable <- cbind(ctable, "p value" = p)
ctable
```


**置信区间**

我们可以使用`confint`来得到置信区间，有两种方式：

```{r}
# 1. 调用的是MASS中的confint
ci <- confint(fit)
ci
```

```{r}
# 2. 使用近似正态分布进行的估计，略有不同但差别不大
confint.default(fit)
```

**OR值**

直接使用系数可能难以进行解释，所以我们一般来说更喜欢使用OR值，这个可以直接对系数取exp得到：
```{r}
exp(cbind(OR = coef(fit), ci))
```

接下来，我们按照上面提到的解释方法，我们可以得到下面的解释：

* 对于pared，在固定其他因素的条件下，**父母教育水平高的学生有更高的意愿去读研究生（父母教育水平高的学生其选择读研究生vs不太想读研究生或不太想读vs不读的odds是父母教育水平低的学生的2.85倍）。**
* 对于public，在固定其他因素的条件下：**读公立的学生其读研究生的意愿比较低。**
* 对于GPA，在固定其他因素的条件下：**每升高一分，就读研究生的odds就升高1.85倍。**

## 关于比例优势假定

PLA假定了所有分点的odds拥有相同的自变量效应（即自变量系数相同，不包括截距项）。这使得我们只需要一组自变量系数就可以建立模型。对PLA进行检验的方法可以分为两类：假设检验和图形化方法。

### Brant假设检验

> 除了使用Brant假设检验外，还有一种方式是使用VGAM包，其基于似然比检验进行，但整个流程比较麻烦。其详细信息可见[参考3](https://medium.com/evangelinelee/brant-test-for-proportional-odds-in-r-b0b373a93aa2)。

**这里我们推荐使用Brant假设检验来判断是否满足PLA，这也是我认为的首选方案。**其原始论文在[这里](https://www.jstor.org/stable/2532457?origin=crossref&seq=1#page_scan_tab_contents)，有一个R包`brant`实现了它，而且可以和`MASS::polr`完美契合。

```{r}
if (!require(brant)) {
  install.packages("brant")
}
# 使用非常简单，直接将polr对象放进去即可
br_fit <- brant(fit)
br_fit
```

其结果中展示的是对于总体（Omnibus）和每一个系数是否符合PLA的假设检验，其中H0是符合PLA。可以看到，brant认为此数据集是符合PLA的。

```{r}
class(br_fit)
```


### Harrell图形化方法

y因为一些假设检验容易犯假阴性错误（II类错误，见 Harrell 2001 p.335），所以Harrell推荐一种图形化的方式来进行判断，其基本思想是：先假设比例优势假定并不符合，则对于每个分点，我们等价于拟合一个二分类的logistic回归，然后我们查看这些logistic回归得到的系数值在相同的自变量上是否相同或相近。

为了稳健性，我们进行二分类logistic回归的时候只是用一个变量，如我们检验变量pared是否符合PLA：

```{r}
# 1. 对于切点2
glm(I(as.numeric(apply) >= 2) ~ pared, family="binomial", data = ologit) # I(...)放在公式中表示，...是需要先执行的R代码，使用执行后的结果作为公式的一部分

```

```{r}
# 2. 对于切点3
glm(I(as.numeric(apply) >= 3) ~ pared, family="binomial", data = ologit)
```

我们需要需要做的是比较上面两个模型的系数值，即1.14和1.09差别是否足够小。

以上的方法进行了两次单独的logistic回归来得到我们需要的系数，但其实有更加优雅的方式来一次性得到它们。因为我们的logistic模型中只有一个自变量，我们有下面的模型：
$$logit(P(Y\ge j))=\beta_{0j}+\beta_{1j}x$$
当$x$是二分类变量时，我们可以得到：
$$logit(P(Y\ge j|x=0))=\beta_{0j}$$
$$logit(P(Y\ge j|x=1))=\beta_{0j}+\beta_{1j}$$
$$\beta_{1j}=logit(P(Y\ge j|x=1))-logit(P(Y\ge j|x=0))$$
所以，我们可以分别统计$x=1$和$x=0$是$Y\ge j$的频率，两者相减即可。当然，我们也可以从另外的角度来进行比较，我们可以计算
$$logit(P(Y\ge j+1|x=0))-logit(P(Y\ge j|x=0))=\beta_{0(j+1)}-\beta_{0j}$$
和
$$logit(P(Y\ge j+1|x=1))-logit(P(Y\ge j|x=1))=\beta_{0(j+1)}-\beta_{0j}+(\beta_{1(j+1)}-\beta_{1j})$$
进行比较，若两者比较相似，意味着$\beta_{1(j+1)}-\beta_{1j}\approx0$，也就是PLA成立。某种意义上来说，这样计算更加方便。

下面借助`Hmisc::summary.formula`来快速实现上面操作：

```{r}
library(Hmisc)  # summary.formula在Hmisc中
sf <- function(y) {
  c(
    "Y>=1" = qlogis(mean(y >= 1)),  # qlogis是logistic分布的分位数函数，是logistic的累积分布函数sigmoid的反函数，即logit函数
    "y>=2" = qlogis(mean(y >= 2)),
    "y>=3" = qlogis(mean(y >= 3))
  )
}

# Hmisc中的summary，详细信息见其帮助文档
s <- with(ologit, summary(as.numeric(apply) ~ pared + public + gpa, fun=sf))
s
```

> $0.765-(-0.378)=1.143$和$-1.347-(-2.441)=1.094$正好是我们上面两个logistic回归得到的系数值。

```{r}
# 这里计算的实际上不是beta_1j值，而是携带有beta_1j的项，如果No和Yes的此项比较相似，则可以认为beta_1j和beta_1(j+1)也比较相似
s[, 4] <- s[, 4] - s[, 3]
s[, 3] <- s[, 3] - s[, 3]
s
```

然后我们可以将上面的数值绘制成图像：

```{r}
class(s)
```

```{r fig.height=8, fig.width=8}
# 这里的s
plot(s, which = 1:3, pch = 1:3, xlab = "logit", main = " ", xlim = range(s[,3:4]))
```

我们可以看到，对于变量pared，PLA是成立的；而对于另外两个变量public和gpa，PLA可能是不成立的。我们需要重新考虑是否要用POM对该数据建模。


## 预测概率

假设我们的模型符合PLA，则我们可以通过下面的方式得到每个样本在Y的不同level上的概率：

```{r}
newdat <- data.frame(
  pared = rep(0:1, 200),
  public = rep(0:1, each = 200),
  gpa = rep(seq(from = 1.9, to = 4, length.out = 100), 4)
)
newdat <- cbind(newdat, predict(fit, newdat, type = "probs"))
head(newdat)
```

现在我们可以把图绘制出来：

```{r}
library(tidyr)
lnewdat <- pivot_longer(newdat, 4:6, names_to = "Level", values_to = "Probability")
head(lnewdat)
```

```{r fig.height=8, fig.width=8}
library(ggplot2)
ggplot(lnewdat, aes(x = gpa, y = Probability, colour = Level)) +
  geom_line() + facet_grid(pared ~ public, labeller="label_both") +
  theme_bw()
```

## 注意事项

1. 因为使用了极大似然估计，所以需要有较大的**样本量**，至少应超过OLS回归。
2. 最好在运行模型前，做一下分类变量的**列联表**，如果有一些组合的样本很少，则可能拟合模型变得非常不稳定。
3. 没有OLS中的R2，只有一个**伪R2**。

# 参考

1. [R Data Analysis Examples Ordinal Logistic Regression](https://stats.idre.ucla.edu/r/dae/ordinal-logistic-regression/)
2. [How do I interpret the coefficients in an ordinal logistic regression in R](https://stats.idre.ucla.edu/r/faq/ologit-coefficients/)
3. [Ordinal Logistic Regression and its Assumptions - Brant Test](https://medium.com/evangelinelee/brant-test-for-proportional-odds-in-r-b0b373a93aa2)
